# stackend

![Version: 2.1.0](https://img.shields.io/badge/Version-2.1.0-informational?style=flat-square) ![Type: application](https://img.shields.io/badge/Type-application-informational?style=flat-square) ![AppVersion: v1.0.1](https://img.shields.io/badge/AppVersion-v1.0.1-informational?style=flat-square)

A Helm chart for StackAI Backend Services

**Homepage:** <https://stackai.com>

## Maintainers

| Name | Email | Url |
| ---- | ------ | --- |
| StackAI Team | <support@stackai.com> |  |

## Source Code

* <https://github.com/stackai/stackai>

## Values

| Key | Type | Default | Description |
|-----|------|---------|-------------|
| affinity | object | `{}` |  |
| autoscaling.enabled | bool | `false` |  |
| autoscaling.maxReplicas | int | `100` |  |
| autoscaling.minReplicas | int | `1` |  |
| autoscaling.targetCPUUtilizationPercentage | int | `80` |  |
| celeryImage.pullPolicy | string | `"IfNotPresent"` |  |
| celeryImage.repository | string | `"stackai.azurecr.io/stackai/stackend-celery-worker"` |  |
| celeryImage.tag | string | `"v1.0.1"` |  |
| celeryResources.limits.cpu | string | `"1000m"` |  |
| celeryResources.limits.memory | string | `"2Gi"` |  |
| celeryResources.requests.cpu | string | `"500m"` |  |
| celeryResources.requests.memory | string | `"1Gi"` |  |
| configMaps.embeddingsConfig | string | `"#\n# EMBEDDING PROVIDER CONFIGURATION\n# \n# This document contains the configuration of the embedding providers that will be available when using StackAI\n# \n# How to use this file:\n# You may add as many local embedding models as you need, the only requirement for the models to work is that they must\n# be compatible with the OpenAI embeddings API.\n#\n# To add a new local embedding model:\n# 1. Uncomment the [embeddings.providers.local] section\n# 2. Copy one of the sample local model configurations and paste it. Uncomment it after that.\n# 3. Set the key to the model in the [embeddings.providers.local.REPLACE_WITH_THE_MODEL_NAME]\n# 4. Set the api_url to the base URL of the OpenAI compatible API.\n# 5. Set the api_key\n# 6. Set the model_name to the name of the embedding model. IMPORTANT: The model names must be unique.\n# 7. Set the context_window to the maximum number of tokens that the embedding model can handle\n# 8. Change the [embeddings.default_model] section to use your model (Opntional if you have specified an OpenAI key)\n#    a) Locate the [embeddings.default_model] section in this file\n#    b) Change the provider to the provider you want to use by default, if you want to use a local model, change it to `provider = \"local\"`\n#    c) Change the model_name to the name of the model you want to use by default\n#\n\n# EXAMPLE LOCAL EMBEDDING MODEL CONFIGURATION:\n\n#[embeddings.providers.local]\n#[embeddings.providers.local.m2_bert_80m]\n#api_url = \"https://api.together.xyz/v1\"\n#api_key = \"example_api_key\"\n#model_name = \"togethercomputer/m2-bert-80M-8k-retrieval\"\n#context_window = 8192\n\n#[embeddings.providers.local.another_model]\n#api_url = \"https://api.together.xyz/v1\"\n#api_key = \"another_api_key\"\n#model_name = \"another_model_name\"\n#context_window = 4096\n\n[embeddings.default_model]\nmodel_name = \"text-embedding-ada-002\"\nprovider = \"openai\"\n\n[embeddings.providers.openai]\n[embeddings.providers.openai.text-embedding-3-large]\nmodel_name=\"text-embedding-3-large\"\ncontext_window=8192\n\n[embeddings.providers.openai.text-embedding-3-small]\nmodel_name=\"text-embedding-3-small\"\ncontext_window=8192\n\n[embeddings.providers.openai.text-embedding-ada-002]\nmodel_name=\"text-embedding-ada-002\"\ncontext_window=8192\n\n\n[embeddings.providers.azure]\n[embeddings.providers.azure.azure-text-embedding-ada-002]\nmodel_name=\"azure-text-embedding-ada-002\"\ncontext_window=8192\n\n[embeddings.providers.bedrock]\n[embeddings.providers.bedrock.titan-embed-text-v1]\nmodel_name=\"amazon.titan-embed-text-v1\"\ncontext_window=8000\n\n[embeddings.providers.bedrock.embed-english-v3]\nmodel_name=\"cohere.embed-english-v3\"\ncontext_window=512\n\n[embeddings.providers.bedrock.embed-multilingual-v3]\nmodel_name=\"cohere.embed-multilingual-v3\"\ncontext_window=512\n"` |  |
| configMaps.llmAzureConfig | string | `"[llms.providers.Azure]\nplan_required = \"enterprise\"\ndocs_url_slug = \"azure-openai\"\nhosted_by = \"Stack AI\"\ndisplay_order = 11\n\n[llms.providers.Azure.\"gpt-5-2025-08-07\"]\nmodel_name = \"GPT-5\"\ndescription = \"GPT-5 is a large reasoning model, providing high intelligence at the same cost and latency targets.\"\ncontext_window = 400000\nmax_output_tokens = 128000\nrating_reasoning = 3\nrating_speed = 1\nrating_context = 3\ndate = \"August 7, 2025\"\nwarning = \"Latest and Smartest\"\nhas_json_format = true\nhas_json_schema = true\nhas_vision = true\nsupported_media_types = [\"image/jpeg\", \"image/png\", \"image/webp\", \"image/gif\"]\nhas_function_calling = true\n\n[llms.providers.Azure.\"gpt-5-mini-2025-08-07\"]\nmodel_name = \"GPT-5 mini\"\ndescription = \"GPT-5 mini is a small reasoning model, providing high intelligence at the same cost and latency targets.\"\ncontext_window = 400000\nmax_output_tokens = 128000\nrating_reasoning = 2\nrating_speed = 3\nrating_context = 3\ndate = \"August 7, 2025\"\nwarning = \"Latest and Faster\"\nhas_json_format = true\nhas_json_schema = true\nhas_vision = true\nsupported_media_types = [\"image/jpeg\", \"image/png\", \"image/webp\", \"image/gif\"]\nhas_function_calling = true\n\n[llms.providers.Azure.\"gpt-5-nano-2025-08-07\"]\nmodel_name = \"GPT-5 nano\"\ndescription = \"GPT-5 nano is the fastest version of GPT-5. Great for summarization and classification tasks.\"\ncontext_window = 400000\nmax_output_tokens = 128000\nrating_reasoning = 2\nrating_speed = 3\nrating_context = 2\ndate = \"August 7, 2025\"\nwarning = \"Latest and Fastest\"\nhas_json_format = true\nhas_json_schema = true\nhas_vision = true\nsupported_media_types = [\"image/jpeg\", \"image/png\", \"image/webp\", \"image/gif\"]\nhas_function_calling = true\n\n[llms.providers.Azure.\"gpt-4.1\"]\nmodel_name = \"GPT 4.1\"\ndescription = \"The latest version of GPT-4.1, with improved reasoning and function calling capabilities.\"\ncontext_window = 200000\nmax_output_tokens = 32768\nrating_reasoning = 3\nrating_speed = 2\nrating_context = 3\nwarning = \"Smartest\"\ndate = \"April 23, 2025\"\nhas_json_format = true\nhas_json_schema = true\nhas_vision = true\nsupported_media_types = [\"image/jpeg\", \"image/png\", \"image/webp\", \"image/gif\"]\nhas_function_calling = true\n\n[llms.providers.Azure.o4-mini]\nmodel_name = \"o4 Mini\"\ndescription = \"o4-mini is a small reasoning model, providing high intelligence at the same cost and latency targets of o1-mini.\"\ncontext_window = 200000\nmax_output_tokens = 100000\nrating_reasoning = 3\nrating_speed = 2\nrating_context = 3\nwarning = \"Smartest\"\ndate = \"April 23, 2025\"\nhas_json_format = true\nhas_json_schema = true\nhas_vision = true\nsupported_media_types = [\"image/jpeg\", \"image/png\", \"image/webp\", \"image/gif\"]\nhas_function_calling = true\n\n[llms.providers.Azure.o3-mini]\nmodel_name = \"o3 Mini\"\ndescription = \"o3-mini is a small reasoning model, providing high intelligence at the same cost and latency targets of o1-mini.\"\ncontext_window = 200000\nmax_output_tokens = 100000\nrating_reasoning = 3\nrating_speed = 2\nrating_context = 3\ndate = \"January 31, 2025\"\nhas_json_format = true\nhas_json_schema = true\nhas_vision = false\nsupported_media_types = []\nhas_function_calling = true\n\n[llms.providers.Azure.o1-mini]\nmodel_name = \"o1 Mini\"\ndescription = \"Optimized for STEM reasoning, especially math and coding. Performs comparably to the larger o1 model on AIME and Codeforces, but runs 80% faster. Ideal for STEM applications, less effective for non-STEM tasks.\"\ncontext_window = 128000\nmax_output_tokens = 65536\nrating_reasoning = 3\nrating_speed = 2\nrating_context = 2\ndate = \"September 12, 2024\"\nhas_json_format = true\nhas_json_schema = true\nhas_vision = false\nsupported_media_types = []\nhas_function_calling = true\n\n[llms.providers.Azure.gpt-4o-mini]\nmodel_name = \"GPT-4o Mini\"\ndescription = \"Light-weight version of GPT 4o. Enables a broad range of tasks with its low cost and latency (perfect for everyday tasks). Supports vision and surpasses GPT-3.5 Turbo performance.\"\ncontext_window = 128000\nmax_output_tokens = 16384\ntraining_data = \"Up to October 2023\"\nrating_reasoning = 3\nrating_speed = 3\nrating_context = 2\nwarning = \"Fastest\"\ndate = \"July 18, 2024\"\nhas_json_format = true\nhas_json_schema = true\nhas_vision = true\nsupported_media_types = [\"image/jpeg\", \"image/png\", \"image/webp\", \"image/gif\"]\nhas_function_calling = true\n\n[llms.providers.Azure.gpt-40]\nmodel_name = \"GPT-4o\"\ndescription = \"Flagship model from OpenAI's GPT-4 series, best for complex tasks. Supports vision & structured outputs.\"\ncontext_window = 128000\nmax_output_tokens = 4096\ntraining_data = \"Up to October 2023\"\nrating_reasoning = 3\nrating_speed = 3\nrating_context = 2\nwarning = \"Smartest\"\ndate = \"May 13, 2024\"\nhas_json_format = true\nhas_json_schema = false\nhas_vision = true\nsupported_media_types = [\"image/jpeg\", \"image/png\", \"image/webp\", \"image/gif\"]\nhas_function_calling = true\n\n[llms.providers.Azure.gpt-4-turbo]\nmodel_name = \"GPT-4 Turbo\"\ndescription = \"GPT-4 model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens.\"\ncontext_window = 128000\nmax_output_tokens = 4096\ntraining_data = \"Up to Dec 2023\"\nrating_reasoning = 3\nrating_speed = 2\nrating_context = 3\ndate = \"November 6, 2023\"\nhas_json_format = true\nhas_json_schema = false\nhas_vision = true\nsupported_media_types = [\"image/jpeg\", \"image/png\", \"image/webp\", \"image/gif\"]\nhas_function_calling = true\n\n[llms.providers.Azure.default]\nmodel_id = \"gpt-4o\"\n"` |  |
| configMaps.llmBedrockConfig | string | `"[llms.providers.Bedrock]\nplan_required = \"enterprise\"\nhosted_by = \"Stack AI\"\ndisplay_order = 12\n\n[llms.providers.Bedrock.\"anthropic.claude-opus-4-20250514-v1:0\"]\nmodel_name = \"Claude 4 Opus\"\ndescription = \"Anthropic's most powerful Claude 4 model, optimized for complex reasoning and challenging tasks.\"\ncontext_window = 200000\nrating_reasoning = 3\nrating_speed = 2\nrating_context = 3\nwarning = \"Large Context\"\ndate = \"Juen 1, 2025\"\nhas_json_format = false\nhas_json_schema = false\nhas_vision = true\nsupported_media_types = []\nhas_function_calling = true\n\n[llms.providers.Bedrock.\"anthropic.claude-sonnet-4-20250514-v1:0\"]\nmodel_name = \"Claude 4 Sonnet\"\ndescription = \"Anthropic's Claude 4 Sonnet model, optimized for efficient performance.\"\ncontext_window = 200000\nrating_reasoning = 3\nrating_speed = 3\nrating_context = 3\nwarning = \"Large Context\"\ndate = \"June 1, 2025\"\nhas_json_format = false\nhas_json_schema = false\nhas_vision = true\nsupported_media_types = []\nhas_function_calling = true\n\n[llms.providers.Bedrock.\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"]\nmodel_name = \"Claude 3.7 Sonnet\"\ndescription = \"Anthropic's Claude 3.7 Sonnet model, optimized for efficient performance.\"\ncontext_window = 200000\nrating_reasoning = 3\nrating_speed = 2\nrating_context = 3\nwarning = \"Large Context\"\ndate = \"April 4, 2025\"\nhas_json_format = false\nhas_json_schema = false\nhas_vision = true\nsupported_media_types = []\nhas_function_calling = true\n\n[llms.providers.Bedrock.\"anthropic.claude-3-5-haiku-20241022-v1:0\"]\nmodel_name = \"Claude 3.5 Haiku\"\ndescription = \"Next generation of Anthropic's fastest and most cost-effective model. It is optimal for use cases where speed and affordability matter. It has been optimized for: code completions, interactive chat bots, data extraction and labeling and real-time content moderation.\"\ncontext_window = 200000\nrating_reasoning = 3\nrating_speed = 3\nrating_context = 3\nwarning = \"New\"\ndate = \"October 22, 2024\"\nhas_json_format = false\nhas_json_schema = false\nhas_vision = false\nsupported_media_types = []\nhas_function_calling = true\n\n[llms.providers.Bedrock.\"anthropic.claude-3-5-sonnet-20240620-v1:0\"]\nmodel_name = \"Claude 3.5 Sonnet\"\ndescription = \"Anthropic's Claude 3.5 Sonnet model, optimized for efficient performance.\"\ncontext_window = 200000\nrating_reasoning = 2\nrating_speed = 3\nrating_context = 3\nwarning = \"Large Context\"\ndate = \"June 20, 2024\"\nhas_json_format = false\nhas_json_schema = false\nhas_vision = true\nsupported_media_types = [\"image/jpeg\", \"image/png\", \"image/webp\", \"image/gif\"]\nhas_function_calling = true\n\n[llms.providers.Bedrock.\"anthropic.claude-3-haiku-20240307-v1:0\"]\nmodel_name = \"Claude 3 Haiku\"\ndescription = \"Anthropic's Claude 3 fastest model that can execute lightweight actions.\"\ncontext_window = 200000\nrating_reasoning = 2\nrating_speed = 3\nrating_context = 3\nwarning = \"Large Context\"\ndate = \"March 4, 2024\"\nhas_json_format = false\nhas_json_schema = false\nhas_vision = true\nsupported_media_types = [\"image/jpeg\", \"image/png\", \"image/webp\", \"image/gif\"]\nhas_function_calling = true\n\n[llms.providers.Bedrock.\"anthropic.claude-3-sonnet-20240229-v1:0\"]\nmodel_name = \"Claude 3 Sonnet\"\ndescription = \"Anthropic's best combination of performance and speed for efficient, high-throughput tasks using Claude 3 architecture.\"\ncontext_window = 200000\nrating_reasoning = 2\nrating_speed = 3\nrating_context = 3\nwarning = \"Large Context\"\ndate = \"March 4, 2024\"\nhas_json_format = false\nhas_json_schema = false\nhas_vision = true\nsupported_media_types = [\"image/jpeg\", \"image/png\", \"image/webp\", \"image/gif\"]\nhas_function_calling = true\n\n[llms.providers.Bedrock.\"deepseek.r1-v1:0\"]\nmodel_name = \"DeepSeek R1\"\ndescription = \"DeepSeek R1 model available on AWS Bedrock.\"\ncontext_window = 200000\nrating_reasoning = 3\nrating_speed = 2\nrating_context = 3\ndate = \"April 25, 2025\"\nhas_json_format = false\nhas_json_schema = false\nhas_vision = true\nsupported_media_types = []\nhas_function_calling = true\n\n[llms.providers.Bedrock.\"amazon.nova-pro-v1:0\"]\nmodel_name = \"Nova Pro\"\ndescription = \"Amazon's Nova Pro model available on AWS Bedrock.\"\ncontext_window = 300000\nrating_reasoning = 3\nrating_speed = 3\nrating_context = 3\ndate = \"April 25, 2025\"\nhas_json_format = false\nhas_json_schema = false\nhas_vision = true\nsupported_media_types = []\nhas_function_calling = true\n\n[llms.providers.Bedrock.\"cohere.command-r-plus-v1:0\"]\nmodel_name = \"Command R+ v1.0\"\ndescription = \"Cohere's Command R+ v1.0 model available on AWS Bedrock.\"\ncontext_window = 4096\nrating_reasoning = 2\nrating_speed = 2\nrating_context = 1\ndate = \"June 15, 2023\"\nhas_json_format = false\nhas_json_schema = false\nhas_vision = false\nsupported_media_types = []\nhas_function_calling = false\n\n[llms.providers.Bedrock.\"cohere.command-r-v1:0\"]\nmodel_name = \"Command R v1.0\"\ndescription = \"Cohere's Command R v1.0 model available on AWS Bedrock.\"\ncontext_window = 4096\nrating_reasoning = 2\nrating_speed = 2\nrating_context = 1\ndate = \"June 15, 2023\"\nhas_json_format = false\nhas_json_schema = false\nhas_vision = false\nsupported_media_types = []\nhas_function_calling = false\n\n[llms.providers.Bedrock.\"meta.llama3-70b-instruct-v1:0\"]\nmodel_name = \"Llama 3 70B\"\ndescription = \"Llama 3 70B model available on AWS Bedrock.\"\ncontext_window = 128000\nrating_reasoning = 2\nrating_speed = 2\nrating_context = 2\ndate = \"July 23, 2024\"\nhas_json_format = false\nhas_json_schema = false\nhas_vision = false\nsupported_media_types = []\nhas_function_calling = false\n\n[llms.providers.Bedrock.\"us.meta.llama3-2-1b-instruct-v1:0\"]\nmodel_name = \"Llama 3.2 1B Instruct\"\ndescription = \"Llama 3.2 1B Instruct model available on AWS Bedrock.\"\ncontext_window = 128000\nrating_reasoning = 2\nrating_speed = 2\nrating_context = 2\ndate = \"2024-09-25\"\nhas_json_format = false\nhas_json_schema = false\nhas_vision = false\nsupported_media_types = []\nhas_function_calling = false\n\n[llms.providers.Bedrock.\"us.meta.llama3-2-3b-instruct-v1:0\"]\nmodel_name = \"Llama 3.2 3B Instruct\"\ndescription = \"Llama 3.2 3B Instruct model available on AWS Bedrock.\"\ncontext_window = 128000\nrating_reasoning = 2\nrating_speed = 2\nrating_context = 2\ndate = \"2024-09-25\"\nhas_json_format = false\nhas_json_schema = false\nhas_vision = false\nsupported_media_types = []\nhas_function_calling = false\n\n[llms.providers.Bedrock.default]\nmodel_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n"` |  |
| configMaps.llmConfig | string | `"# LLM PROVIDER CONFIGURATION\n#\n# This document contains the configuration of the LLM providers that will be available when using StackAI\n# \n# How to use this file:\n#\n# 1. If you want to use local LLM models, set up the `stackend/llm_local_config.toml` file first.\n#\n# 2. Adjust the llms that are used for each use case:\n# For each of the use cases, you may change the llm provider and model as you need.\n# If you only have local llms available, change the provider to `provider = \"Local\"` (yes, capital L) and adjust\n# the model_name to one you have set up in the `stackend/llm_local_config.toml` file.\n# \n# 3. Set the default llm in the [default_llm] section.\n# This llm will be used by some nodes when the user does not specify which llm to use.\n# If you are using external llm providers, change the provider to the name of the provider.\n# If you are using local llms, change the provider to `provider = \"Local\"` (yes, capital L) and adjust\n# the model_name to one you have set up in the `stackend/llm_local_config.toml` file.\n\n[default_llm]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n\n[use_cases.routing_node]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n\n[use_cases.rag_node]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n\n[use_cases.csv_search]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n\n[use_cases.airtable_search]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n\n[use_cases.airtable_writer]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n\n[use_cases.guardrails]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n\n[use_cases.retrieval_util]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n\n[use_cases.break_to_subquestions]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n\n[use_cases.text_2_sql_node]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n\n[use_cases.hyde_transformer]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n\n[use_cases.keyword_transformer]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n\n[use_cases.llm_document_processor]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n\n[use_cases.url_tool]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n\n[use_cases.beautiful_soup_tool]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n\n[use_cases.dataframe_tool]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n\n[use_cases.llm_tool]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n\n[use_cases.serp_api_tool]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n"` |  |
| configMaps.llmLocalConfig | string | `"# Local LLM models configuration\n#\n# This file contains the configuration for the local LLM models that will be available when using StackAI.\n#\n# How to use this file:\n# The set up process has two steps:\n# 1. Add as many local LLM models as you intend to use (see commented example below)\n# 2. Set up the default model in the [llms.providers.Local.default] section.\n#    IMPORTANT: The model_id variable should be the name of the model in the api. For instance, if you were\n#    to set up the sample llm below as your default, you should set `model_id` to \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n#    rather than \"Meta Llama 3.1 Turbo\"\n#\n# Common issues solved:\n# - The provider name for the local llms is \"Local\", not \"local\"\n\n\n[llms.providers.Local]\nplan_required = \"enterprise\"\n\n\n# REFERENCE LOCAL MODEL CONFIGURATION:\n#\n# You may add as many local LLM models as you need.\n# To do so, please use the configuration below as a reference and adjust it to your needs\n# The most important values to set up are:\n#\n# - model name in the api: This needs to be put in the toml header [llms.providers.Local.\"YOUR MODEL NAME IN YOUR API\"]\n# - endpoint_base_url: The base URL of the OpenAI compatible API. This has to be set up in the toml body.\n# - api_key: The API key for the OpenAI compatible API. This has to be set up in the toml body.\n#\n#\n# [llms.providers.Local.\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"]\n# endpoint_base_url = \"https://api.together.xyz/v1\"\n# api_key = \"example api key\"\n# model_name = \"Meta Llama 3.1 Turbo\"\n# description = \"This is a dummy configuration model that you can use as reference to create your own configuration.\"\n# context_window = 1024\n# reasoning = 1\n# speed = 1\n# context = 1\n# date = \"\"\n# has_json_format = false\n# has_json_schema = false\n# has_vision = false\n# supported_media_types = []\n# has_function_calling = false\n\n[llms.providers.Local.generic_local]\nmodel_name = \"OpenAI API Compatible Model\"\nendpoint_base_url = \"https://api.together.xyz/v1\"\napi_key = \"sk-1234-5678-abcd\"\ndescription = \"A local model for testing and development. Go into the node settings and set up the endpoint base url and api key of the node for it to work.\"\ncontext_window = 128000\nreasoning = 1\nspeed = 1\ncontext = 1\ndate = \"\"\nhas_json_format = false\nhas_json_schema = false\nhas_vision = false\nsupported_media_types = []\nhas_function_calling = false\n\n\n# Remember to adjust this as well!\n[llms.providers.Local.default]\nprovider = \"Local\"\nmodel_id = \"generic_local\"\n"` |  |
| configMaps.llmOpenaiConfig | string | `"[llms.providers.OpenAI]\ndefault_provider = true\ndocs_url_slug = \"openai\"\ndisplay_order = 1\n\n\n[llms.providers.OpenAI.\"o3\"]\nmodel_name = \"o3\"\ndescription = \"o3 is a large reasoning model, providing high intelligence at the same cost and latency targets.\"\ncontext_window = 200000\nrating_reasoning = 3\nrating_speed = 1\nrating_context = 3\ndate = \"June 10, 2025\"\nhas_json_format = true\nhas_json_schema = true\nhas_vision = false\nsupported_media_types = []\nhas_function_calling = true\n\n[llms.providers.OpenAI.\"o4-mini\"]\nmodel_name = \"o4 Mini\"\ndescription = \"o4-mini is a small reasoning model, providing high intelligence at the same cost and latency targets of o1-mini.\"\ncontext_window = 200000\nrating_reasoning = 3\nrating_speed = 2\nrating_context = 3\ndate = \"April 16, 2025\"\nhas_json_format = true\nhas_json_schema = true\nhas_vision = false\nsupported_media_types = []\nhas_function_calling = true\n\n\n[llms.providers.OpenAI.\"gpt-4.1-2025-04-14\"]\nmodel_name = \"GPT 4.1\"\ndescription = \"GPT 4.1 model. Supports vision and structured outputs.\"\ncontext_window = 1280000\nrating_reasoning = 3\nrating_speed = 2\nrating_context = 3\nwarning = \"Smartest\"\ndate = \"April 14, 2025\"\nhas_json_format = true\nhas_json_schema = true\nhas_vision = true\nsupported_media_types = [\"image/jpeg\", \"image/png\", \"image/webp\", \"image/gif\"]\nhas_function_calling = true\n\n[llms.providers.OpenAI.\"gpt-4.5-preview\"]\nmodel_name = \"GPT 4.5 preview\"\ndescription = \"GPT 4.5 preview model. Supports vision and structured outputs.\"\ncontext_window = 128000\nrating_reasoning = 3\nrating_speed = 2\nrating_context = 3\nwarning = \"Smartest\"\ndate = \"February 28, 2025\"\nhas_json_format = true\nhas_json_schema = false\nhas_vision = true\nsupported_media_types = [\"image/jpeg\", \"image/png\", \"image/webp\", \"image/gif\"]\nhas_function_calling = true\n\n[llms.providers.OpenAI.gpt-4o-mini]\nmodel_name = \"GPT-4o Mini\"\ndescription = \"Light-weight version of GPT 4o. Enables a broad range of tasks with its low cost and latency (perfect for everyday tasks). Supports vision and surpasses GPT-3.5 Turbo performance.\"\ncontext_window = 128000\ntraining_data = \"Up to October 2023\"\nrating_reasoning = 3\nrating_speed = 3\nrating_context = 2\nwarning = \"Fastest\"\ndate = \"July 18, 2024\"\nhas_json_format = true\nhas_json_schema = true\nhas_vision = true\nsupported_media_types = [\"image/jpeg\", \"image/png\", \"image/webp\", \"image/gif\"]\nhas_function_calling = true\n\n[llms.providers.OpenAI.gpt-4o]\nmodel_name = \"GPT-4o\"\ndescription = \"Flagship model from OpenAI's GPT-4 series, best for complex tasks. Supports vision & structured outputs.\"\ncontext_window = 128000\ntraining_data = \"Up to October 2023\"\nrating_reasoning = 3\nrating_speed = 3\nrating_context = 2\ndate = \"May 13, 2024\"\nhas_json_format = true\nhas_json_schema = true\nhas_vision = true\nsupported_media_types = [\"image/jpeg\", \"image/png\", \"image/webp\", \"image/gif\"]\nhas_function_calling = true\n\n[llms.providers.OpenAI.o3-mini]\nmodel_name = \"o3 Mini\"\ndescription = \"o3-mini is a small reasoning model, providing high intelligence at the same cost and latency targets of o1-mini.\"\ncontext_window = 200000\nrating_reasoning = 3\nrating_speed = 2\nrating_context = 3\ndate = \"January 31, 2025\"\nhas_json_format = true\nhas_json_schema = true\nhas_vision = false\nsupported_media_types = []\nhas_function_calling = true\n\n[llms.providers.OpenAI.gpt-4o-2024-08-06]\nmodel_name = \"GPT-4o 2024-08-06 Snapshot\"\ndescription = \"Flagship model from OpenAI's GPT-4 series, best for complex tasks. Supports vision.\"\ncontext_window = 128000\ntraining_data = \"Up to Apr 2023\"\nrating_reasoning = 3\nrating_speed = 2\nrating_context = 2\ndate = \"August 6, 2024\"\nhas_json_format = true\nhas_json_schema = true\nhas_vision = true\nsupported_media_types = [\"image/jpeg\", \"image/png\", \"image/webp\", \"image/gif\"]\nhas_function_calling = true\n\n[llms.providers.OpenAI.o1-mini]\nmodel_name = \"o1 Mini\"\ndescription = \"Optimized for STEM reasoning, especially math and coding. Performs comparably to the larger o1 model on AIME and Codeforces, but runs 80% faster. Ideal for STEM applications, less effective for non-STEM tasks.\"\ncontext_window = 128000\nrating_reasoning = 3\nrating_speed = 2\nrating_context = 2\ndate = \"September 12, 2024\"\nhas_json_format = false\nhas_json_schema = false\nhas_vision = false\nsupported_media_types = []\nhas_function_calling = false\n\n[llms.providers.OpenAI.default]\nmodel_id = \"gpt-4.1-2025-04-14\"\n"` |  |
| env | string | `nil` |  |
| fullnameOverride | string | `""` |  |
| healthCheck.enabled | bool | `true` |  |
| healthCheck.failureThreshold | int | `3` |  |
| healthCheck.initialDelaySeconds | int | `30` |  |
| healthCheck.periodSeconds | int | `10` |  |
| healthCheck.successThreshold | int | `1` |  |
| healthCheck.timeoutSeconds | int | `5` |  |
| image.pullPolicy | string | `"IfNotPresent"` |  |
| image.repository | string | `"stackai.azurecr.io/stackai/stackend-backend"` |  |
| image.tag | string | `"v1.0.1"` |  |
| imagePullSecrets | list | `[]` |  |
| ingress.annotations | object | `{}` |  |
| ingress.className | string | `""` |  |
| ingress.enabled | bool | `false` |  |
| ingress.hosts[0].host | string | `"stackend.local"` |  |
| ingress.hosts[0].paths[0].path | string | `"/"` |  |
| ingress.hosts[0].paths[0].pathType | string | `"Prefix"` |  |
| ingress.tls | list | `[]` |  |
| livenessProbe.enabled | bool | `true` |  |
| livenessProbe.failureThreshold | int | `3` |  |
| livenessProbe.initialDelaySeconds | int | `30` |  |
| livenessProbe.periodSeconds | int | `10` |  |
| livenessProbe.successThreshold | int | `1` |  |
| livenessProbe.timeoutSeconds | int | `5` |  |
| migration.checkInterval | int | `30` |  |
| migration.delaySeconds | int | `600` |  |
| migration.enabled | bool | `true` |  |
| nameOverride | string | `""` |  |
| nodeSelector | object | `{}` |  |
| persistence.accessMode | string | `"ReadWriteOnce"` |  |
| persistence.enabled | bool | `false` |  |
| persistence.size | string | `"8Gi"` |  |
| persistence.storageClass | string | `""` |  |
| podAnnotations | object | `{}` |  |
| podSecurityContext.fsGroup | int | `2000` |  |
| postMigration.checkInterval | int | `10` |  |
| postMigration.delaySeconds | int | `60` |  |
| postMigration.enabled | bool | `true` |  |
| readinessProbe.enabled | bool | `true` |  |
| readinessProbe.failureThreshold | int | `3` |  |
| readinessProbe.initialDelaySeconds | int | `5` |  |
| readinessProbe.periodSeconds | int | `5` |  |
| readinessProbe.successThreshold | int | `1` |  |
| readinessProbe.timeoutSeconds | int | `3` |  |
| redis.enabled | bool | `true` |  |
| redis.persistence.accessMode | string | `"ReadWriteOnce"` |  |
| redis.persistence.enabled | bool | `false` |  |
| redis.persistence.size | string | `"1Gi"` |  |
| redis.persistence.storageClass | string | `""` |  |
| redisImage.pullPolicy | string | `"IfNotPresent"` |  |
| redisImage.repository | string | `"redis"` |  |
| redisImage.tag | string | `"alpine3.19"` |  |
| redisResources.limits.cpu | string | `"500m"` |  |
| redisResources.limits.memory | string | `"1Gi"` |  |
| redisResources.requests.cpu | string | `"100m"` |  |
| redisResources.requests.memory | string | `"256Mi"` |  |
| replicaCount | int | `1` |  |
| resources.limits.cpu | string | `"2000m"` |  |
| resources.limits.memory | string | `"4Gi"` |  |
| resources.requests.cpu | string | `"1000m"` |  |
| resources.requests.memory | string | `"2Gi"` |  |
| restarter.enabled | bool | `true` |  |
| restarter.image.repository | string | `"docker"` |  |
| restarter.image.tag | string | `"cli"` |  |
| restarter.schedule | string | `"0 7 * * *"` |  |
| securityContext.capabilities.drop[0] | string | `"ALL"` |  |
| securityContext.readOnlyRootFilesystem | bool | `false` |  |
| securityContext.runAsNonRoot | bool | `true` |  |
| securityContext.runAsUser | int | `1000` |  |
| service.inferencePort | int | `8888` |  |
| service.port | int | `8000` |  |
| service.type | string | `"ClusterIP"` |  |
| serviceAccount.annotations | object | `{}` |  |
| serviceAccount.create | bool | `true` |  |
| serviceAccount.name | string | `""` |  |
| tolerations | list | `[]` |  |

----------------------------------------------
Autogenerated from chart metadata using [helm-docs v1.14.2](https://github.com/norwoodj/helm-docs/releases/v1.14.2)
