# Default values for stackend
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: stackai.azurecr.io/stackai/stackend-backend
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "v1.0.1"

celeryImage:
  repository: stackai.azurecr.io/stackai/stackend-celery-worker
  pullPolicy: IfNotPresent
  tag: "v1.0.1"

redisImage:
  repository: redis
  pullPolicy: IfNotPresent
  tag: "alpine3.19"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext:
  fsGroup: 2000

securityContext:
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: false
  runAsNonRoot: true
  runAsUser: 1000

service:
  type: ClusterIP
  port: 8000
  inferencePort: 8888

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: stackend.local
      paths:
        - path: /
          pathType: Prefix
  tls: []
  #  - secretName: stackend-tls
  #    hosts:
  #      - stackend.local

resources:
  limits:
    cpu: 2000m
    memory: 4Gi
  requests:
    cpu: 1000m
    memory: 2Gi

celeryResources:
  limits:
    cpu: 1000m
    memory: 2Gi
  requests:
    cpu: 500m
    memory: 1Gi

redisResources:
  limits:
    cpu: 500m
    memory: 1Gi
  requests:
    cpu: 100m
    memory: 256Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}

# Environment variables
env:
  # Add your environment variables here
  # Example:
  # DATABASE_URL: "postgresql://user:password@host:port/database"
  # REDIS_URL: "redis://redis:6379"

# ConfigMaps for configuration files
configMaps:
  llmConfig: |
    # LLM PROVIDER CONFIGURATION
    #
    # This document contains the configuration of the LLM providers that will be available when using StackAI
    #
    # How to use this file:
    #
    # 1. If you want to use local LLM models, set up the `stackend/llm_local_config.toml` file first.
    #
    # 2. Adjust the llms that are used for each use case:
    # For each of the use cases, you may change the llm provider and model as you need.
    # If you only have local llms available, change the provider to `provider = "Local"` (yes, capital L) and adjust
    # the model_name to one you have set up in the `stackend/llm_local_config.toml` file.
    #
    # 3. Set the default llm in the [default_llm] section.
    # This llm will be used by some nodes when the user does not specify which llm to use.
    # If you are using external llm providers, change the provider to the name of the provider.
    # If you are using local llms, change the provider to `provider = "Local"` (yes, capital L) and adjust
    # the model_name to one you have set up in the `stackend/llm_local_config.toml` file.

    [default_llm]
    provider = "Local"
    model_id = "generic_local"

    [use_cases.routing_node]
    provider = "Local"
    model_id = "generic_local"

    [use_cases.rag_node]
    provider = "Local"
    model_id = "generic_local"

    [use_cases.csv_search]
    provider = "Local"
    model_id = "generic_local"

    [use_cases.airtable_search]
    provider = "Local"
    model_id = "generic_local"

    [use_cases.airtable_writer]
    provider = "Local"
    model_id = "generic_local"

    [use_cases.guardrails]
    provider = "Local"
    model_id = "generic_local"

    [use_cases.retrieval_util]
    provider = "Local"
    model_id = "generic_local"

    [use_cases.break_to_subquestions]
    provider = "Local"
    model_id = "generic_local"

    [use_cases.text_2_sql_node]
    provider = "Local"
    model_id = "generic_local"

    [use_cases.hyde_transformer]
    provider = "Local"
    model_id = "generic_local"

    [use_cases.keyword_transformer]
    provider = "Local"
    model_id = "generic_local"

    [use_cases.llm_document_processor]
    provider = "Local"
    model_id = "generic_local"

    [use_cases.url_tool]
    provider = "Local"
    model_id = "generic_local"

    [use_cases.beautiful_soup_tool]
    provider = "Local"
    model_id = "generic_local"

    [use_cases.dataframe_tool]
    provider = "Local"
    model_id = "generic_local"

    [use_cases.llm_tool]
    provider = "Local"
    model_id = "generic_local"

    [use_cases.serp_api_tool]
    provider = "Local"
    model_id = "generic_local"

  embeddingsConfig: |
    #
    # EMBEDDING PROVIDER CONFIGURATION
    #
    # This document contains the configuration of the embedding providers that will be available when using StackAI
    #
    # How to use this file:
    # You may add as many local embedding models as you need, the only requirement for the models to work is that they must
    # be compatible with the OpenAI embeddings API.
    #
    # To add a new local embedding model:
    # 1. Uncomment the [embeddings.providers.local] section
    # 2. Copy one of the sample local model configurations and paste it. Uncomment it after that.
    # 3. Set the key to the model in the [embeddings.providers.local.REPLACE_WITH_THE_MODEL_NAME]
    # 4. Set the api_url to the base URL of the OpenAI compatible API.
    # 5. Set the api_key
    # 6. Set the model_name to the name of the embedding model. IMPORTANT: The model names must be unique.
    # 7. Set the context_window to the maximum number of tokens that the embedding model can handle
    # 8. Change the [embeddings.default_model] section to use your model (Opntional if you have specified an OpenAI key)
    #    a) Locate the [embeddings.default_model] section in this file
    #    b) Change the provider to the provider you want to use by default, if you want to use a local model, change it to `provider = "local"`
    #    c) Change the model_name to the name of the model you want to use by default
    #

    # EXAMPLE LOCAL EMBEDDING MODEL CONFIGURATION:

    #[embeddings.providers.local]
    #[embeddings.providers.local.m2_bert_80m]
    #api_url = "https://api.together.xyz/v1"
    #api_key = "example_api_key"
    #model_name = "togethercomputer/m2-bert-80M-8k-retrieval"
    #context_window = 8192

    #[embeddings.providers.local.another_model]
    #api_url = "https://api.together.xyz/v1"
    #api_key = "another_api_key"
    #model_name = "another_model_name"
    #context_window = 4096

    [embeddings.default_model]
    model_name = "text-embedding-ada-002"
    provider = "openai"

    [embeddings.providers.openai]
    [embeddings.providers.openai.text-embedding-3-large]
    model_name="text-embedding-3-large"
    context_window=8192

    [embeddings.providers.openai.text-embedding-3-small]
    model_name="text-embedding-3-small"
    context_window=8192

    [embeddings.providers.openai.text-embedding-ada-002]
    model_name="text-embedding-ada-002"
    context_window=8192


    [embeddings.providers.azure]
    [embeddings.providers.azure.azure-text-embedding-ada-002]
    model_name="azure-text-embedding-ada-002"
    context_window=8192

    [embeddings.providers.bedrock]
    [embeddings.providers.bedrock.titan-embed-text-v1]
    model_name="amazon.titan-embed-text-v1"
    context_window=8000

    [embeddings.providers.bedrock.embed-english-v3]
    model_name="cohere.embed-english-v3"
    context_window=512

    [embeddings.providers.bedrock.embed-multilingual-v3]
    model_name="cohere.embed-multilingual-v3"
    context_window=512

  llmLocalConfig: |
    # Local LLM models configuration
    #
    # This file contains the configuration for the local LLM models that will be available when using StackAI.
    #
    # How to use this file:
    # The set up process has two steps:
    # 1. Add as many local LLM models as you intend to use (see commented example below)
    # 2. Set up the default model in the [llms.providers.Local.default] section.
    #    IMPORTANT: The model_id variable should be the name of the model in the api. For instance, if you were
    #    to set up the sample llm below as your default, you should set `model_id` to "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
    #    rather than "Meta Llama 3.1 Turbo"
    #
    # Common issues solved:
    # - The provider name for the local llms is "Local", not "local"


    [llms.providers.Local]
    plan_required = "enterprise"


    # REFERENCE LOCAL MODEL CONFIGURATION:
    #
    # You may add as many local LLM models as you need.
    # To do so, please use the configuration below as a reference and adjust it to your needs
    # The most important values to set up are:
    #
    # - model name in the api: This needs to be put in the toml header [llms.providers.Local."YOUR MODEL NAME IN YOUR API"]
    # - endpoint_base_url: The base URL of the OpenAI compatible API. This has to be set up in the toml body.
    # - api_key: The API key for the OpenAI compatible API. This has to be set up in the toml body.
    #
    #
    # [llms.providers.Local."meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"]
    # endpoint_base_url = "https://api.together.xyz/v1"
    # api_key = "example api key"
    # model_name = "Meta Llama 3.1 Turbo"
    # description = "This is a dummy configuration model that you can use as reference to create your own configuration."
    # context_window = 1024
    # reasoning = 1
    # speed = 1
    # context = 1
    # date = ""
    # has_json_format = false
    # has_json_schema = false
    # has_vision = false
    # supported_media_types = []
    # has_function_calling = false

    [llms.providers.Local.generic_local]
    model_name = "OpenAI API Compatible Model"
    endpoint_base_url = "https://api.together.xyz/v1"
    api_key = "sk-1234-5678-abcd"
    description = "A local model for testing and development. Go into the node settings and set up the endpoint base url and api key of the node for it to work."
    context_window = 128000
    reasoning = 1
    speed = 1
    context = 1
    date = ""
    has_json_format = false
    has_json_schema = false
    has_vision = false
    supported_media_types = []
    has_function_calling = false


    # Remember to adjust this as well!
    [llms.providers.Local.default]
    provider = "Local"
    model_id = "generic_local"

  llmBedrockConfig: |
    [llms.providers.Bedrock]
    plan_required = "enterprise"
    hosted_by = "Stack AI"
    display_order = 12

    [llms.providers.Bedrock."anthropic.claude-opus-4-20250514-v1:0"]
    model_name = "Claude 4 Opus"
    description = "Anthropic's most powerful Claude 4 model, optimized for complex reasoning and challenging tasks."
    context_window = 200000
    rating_reasoning = 3
    rating_speed = 2
    rating_context = 3
    warning = "Large Context"
    date = "Juen 1, 2025"
    has_json_format = false
    has_json_schema = false
    has_vision = true
    supported_media_types = []
    has_function_calling = true

    [llms.providers.Bedrock."anthropic.claude-sonnet-4-20250514-v1:0"]
    model_name = "Claude 4 Sonnet"
    description = "Anthropic's Claude 4 Sonnet model, optimized for efficient performance."
    context_window = 200000
    rating_reasoning = 3
    rating_speed = 3
    rating_context = 3
    warning = "Large Context"
    date = "June 1, 2025"
    has_json_format = false
    has_json_schema = false
    has_vision = true
    supported_media_types = []
    has_function_calling = true

    [llms.providers.Bedrock."us.anthropic.claude-3-7-sonnet-20250219-v1:0"]
    model_name = "Claude 3.7 Sonnet"
    description = "Anthropic's Claude 3.7 Sonnet model, optimized for efficient performance."
    context_window = 200000
    rating_reasoning = 3
    rating_speed = 2
    rating_context = 3
    warning = "Large Context"
    date = "April 4, 2025"
    has_json_format = false
    has_json_schema = false
    has_vision = true
    supported_media_types = []
    has_function_calling = true

    [llms.providers.Bedrock."anthropic.claude-3-5-haiku-20241022-v1:0"]
    model_name = "Claude 3.5 Haiku"
    description = "Next generation of Anthropic's fastest and most cost-effective model. It is optimal for use cases where speed and affordability matter. It has been optimized for: code completions, interactive chat bots, data extraction and labeling and real-time content moderation."
    context_window = 200000
    rating_reasoning = 3
    rating_speed = 3
    rating_context = 3
    warning = "New"
    date = "October 22, 2024"
    has_json_format = false
    has_json_schema = false
    has_vision = false
    supported_media_types = []
    has_function_calling = true

    [llms.providers.Bedrock."anthropic.claude-3-5-sonnet-20240620-v1:0"]
    model_name = "Claude 3.5 Sonnet"
    description = "Anthropic's Claude 3.5 Sonnet model, optimized for efficient performance."
    context_window = 200000
    rating_reasoning = 2
    rating_speed = 3
    rating_context = 3
    warning = "Large Context"
    date = "June 20, 2024"
    has_json_format = false
    has_json_schema = false
    has_vision = true
    supported_media_types = ["image/jpeg", "image/png", "image/webp", "image/gif"]
    has_function_calling = true

    [llms.providers.Bedrock."anthropic.claude-3-haiku-20240307-v1:0"]
    model_name = "Claude 3 Haiku"
    description = "Anthropic's Claude 3 fastest model that can execute lightweight actions."
    context_window = 200000
    rating_reasoning = 2
    rating_speed = 3
    rating_context = 3
    warning = "Large Context"
    date = "March 4, 2024"
    has_json_format = false
    has_json_schema = false
    has_vision = true
    supported_media_types = ["image/jpeg", "image/png", "image/webp", "image/gif"]
    has_function_calling = true

    [llms.providers.Bedrock."anthropic.claude-3-sonnet-20240229-v1:0"]
    model_name = "Claude 3 Sonnet"
    description = "Anthropic's best combination of performance and speed for efficient, high-throughput tasks using Claude 3 architecture."
    context_window = 200000
    rating_reasoning = 2
    rating_speed = 3
    rating_context = 3
    warning = "Large Context"
    date = "March 4, 2024"
    has_json_format = false
    has_json_schema = false
    has_vision = true
    supported_media_types = ["image/jpeg", "image/png", "image/webp", "image/gif"]
    has_function_calling = true

    [llms.providers.Bedrock."deepseek.r1-v1:0"]
    model_name = "DeepSeek R1"
    description = "DeepSeek R1 model available on AWS Bedrock."
    context_window = 200000
    rating_reasoning = 3
    rating_speed = 2
    rating_context = 3
    date = "April 25, 2025"
    has_json_format = false
    has_json_schema = false
    has_vision = true
    supported_media_types = []
    has_function_calling = true

    [llms.providers.Bedrock."amazon.nova-pro-v1:0"]
    model_name = "Nova Pro"
    description = "Amazon's Nova Pro model available on AWS Bedrock."
    context_window = 300000
    rating_reasoning = 3
    rating_speed = 3
    rating_context = 3
    date = "April 25, 2025"
    has_json_format = false
    has_json_schema = false
    has_vision = true
    supported_media_types = []
    has_function_calling = true

    [llms.providers.Bedrock."cohere.command-r-plus-v1:0"]
    model_name = "Command R+ v1.0"
    description = "Cohere's Command R+ v1.0 model available on AWS Bedrock."
    context_window = 4096
    rating_reasoning = 2
    rating_speed = 2
    rating_context = 1
    date = "June 15, 2023"
    has_json_format = false
    has_json_schema = false
    has_vision = false
    supported_media_types = []
    has_function_calling = false

    [llms.providers.Bedrock."cohere.command-r-v1:0"]
    model_name = "Command R v1.0"
    description = "Cohere's Command R v1.0 model available on AWS Bedrock."
    context_window = 4096
    rating_reasoning = 2
    rating_speed = 2
    rating_context = 1
    date = "June 15, 2023"
    has_json_format = false
    has_json_schema = false
    has_vision = false
    supported_media_types = []
    has_function_calling = false

    [llms.providers.Bedrock."meta.llama3-70b-instruct-v1:0"]
    model_name = "Llama 3 70B"
    description = "Llama 3 70B model available on AWS Bedrock."
    context_window = 128000
    rating_reasoning = 2
    rating_speed = 2
    rating_context = 2
    date = "July 23, 2024"
    has_json_format = false
    has_json_schema = false
    has_vision = false
    supported_media_types = []
    has_function_calling = false

    [llms.providers.Bedrock."us.meta.llama3-2-1b-instruct-v1:0"]
    model_name = "Llama 3.2 1B Instruct"
    description = "Llama 3.2 1B Instruct model available on AWS Bedrock."
    context_window = 128000
    rating_reasoning = 2
    rating_speed = 2
    rating_context = 2
    date = "2024-09-25"
    has_json_format = false
    has_json_schema = false
    has_vision = false
    supported_media_types = []
    has_function_calling = false

    [llms.providers.Bedrock."us.meta.llama3-2-3b-instruct-v1:0"]
    model_name = "Llama 3.2 3B Instruct"
    description = "Llama 3.2 3B Instruct model available on AWS Bedrock."
    context_window = 128000
    rating_reasoning = 2
    rating_speed = 2
    rating_context = 2
    date = "2024-09-25"
    has_json_format = false
    has_json_schema = false
    has_vision = false
    supported_media_types = []
    has_function_calling = false

    [llms.providers.Bedrock.default]
    model_id = "anthropic.claude-3-5-sonnet-20240620-v1:0"

  llmAzureConfig: |
    [llms.providers.Azure]
    plan_required = "enterprise"
    docs_url_slug = "azure-openai"
    hosted_by = "Stack AI"
    display_order = 11

    [llms.providers.Azure."gpt-5-2025-08-07"]
    model_name = "GPT-5"
    description = "GPT-5 is a large reasoning model, providing high intelligence at the same cost and latency targets."
    context_window = 400000
    max_output_tokens = 128000
    rating_reasoning = 3
    rating_speed = 1
    rating_context = 3
    date = "August 7, 2025"
    warning = "Latest and Smartest"
    has_json_format = true
    has_json_schema = true
    has_vision = true
    supported_media_types = ["image/jpeg", "image/png", "image/webp", "image/gif"]
    has_function_calling = true

    [llms.providers.Azure."gpt-5-mini-2025-08-07"]
    model_name = "GPT-5 mini"
    description = "GPT-5 mini is a small reasoning model, providing high intelligence at the same cost and latency targets."
    context_window = 400000
    max_output_tokens = 128000
    rating_reasoning = 2
    rating_speed = 3
    rating_context = 3
    date = "August 7, 2025"
    warning = "Latest and Faster"
    has_json_format = true
    has_json_schema = true
    has_vision = true
    supported_media_types = ["image/jpeg", "image/png", "image/webp", "image/gif"]
    has_function_calling = true

    [llms.providers.Azure."gpt-5-nano-2025-08-07"]
    model_name = "GPT-5 nano"
    description = "GPT-5 nano is the fastest version of GPT-5. Great for summarization and classification tasks."
    context_window = 400000
    max_output_tokens = 128000
    rating_reasoning = 2
    rating_speed = 3
    rating_context = 2
    date = "August 7, 2025"
    warning = "Latest and Fastest"
    has_json_format = true
    has_json_schema = true
    has_vision = true
    supported_media_types = ["image/jpeg", "image/png", "image/webp", "image/gif"]
    has_function_calling = true

    [llms.providers.Azure."gpt-4.1"]
    model_name = "GPT 4.1"
    description = "The latest version of GPT-4.1, with improved reasoning and function calling capabilities."
    context_window = 200000
    max_output_tokens = 32768
    rating_reasoning = 3
    rating_speed = 2
    rating_context = 3
    warning = "Smartest"
    date = "April 23, 2025"
    has_json_format = true
    has_json_schema = true
    has_vision = true
    supported_media_types = ["image/jpeg", "image/png", "image/webp", "image/gif"]
    has_function_calling = true

    [llms.providers.Azure.o4-mini]
    model_name = "o4 Mini"
    description = "o4-mini is a small reasoning model, providing high intelligence at the same cost and latency targets of o1-mini."
    context_window = 200000
    max_output_tokens = 100000
    rating_reasoning = 3
    rating_speed = 2
    rating_context = 3
    warning = "Smartest"
    date = "April 23, 2025"
    has_json_format = true
    has_json_schema = true
    has_vision = true
    supported_media_types = ["image/jpeg", "image/png", "image/webp", "image/gif"]
    has_function_calling = true

    [llms.providers.Azure.o3-mini]
    model_name = "o3 Mini"
    description = "o3-mini is a small reasoning model, providing high intelligence at the same cost and latency targets of o1-mini."
    context_window = 200000
    max_output_tokens = 100000
    rating_reasoning = 3
    rating_speed = 2
    rating_context = 3
    date = "January 31, 2025"
    has_json_format = true
    has_json_schema = true
    has_vision = false
    supported_media_types = []
    has_function_calling = true

    [llms.providers.Azure.o1-mini]
    model_name = "o1 Mini"
    description = "Optimized for STEM reasoning, especially math and coding. Performs comparably to the larger o1 model on AIME and Codeforces, but runs 80% faster. Ideal for STEM applications, less effective for non-STEM tasks."
    context_window = 128000
    max_output_tokens = 65536
    rating_reasoning = 3
    rating_speed = 2
    rating_context = 2
    date = "September 12, 2024"
    has_json_format = true
    has_json_schema = true
    has_vision = false
    supported_media_types = []
    has_function_calling = true

    [llms.providers.Azure.gpt-4o-mini]
    model_name = "GPT-4o Mini"
    description = "Light-weight version of GPT 4o. Enables a broad range of tasks with its low cost and latency (perfect for everyday tasks). Supports vision and surpasses GPT-3.5 Turbo performance."
    context_window = 128000
    max_output_tokens = 16384
    training_data = "Up to October 2023"
    rating_reasoning = 3
    rating_speed = 3
    rating_context = 2
    warning = "Fastest"
    date = "July 18, 2024"
    has_json_format = true
    has_json_schema = true
    has_vision = true
    supported_media_types = ["image/jpeg", "image/png", "image/webp", "image/gif"]
    has_function_calling = true

    [llms.providers.Azure.gpt-40]
    model_name = "GPT-4o"
    description = "Flagship model from OpenAI's GPT-4 series, best for complex tasks. Supports vision & structured outputs."
    context_window = 128000
    max_output_tokens = 4096
    training_data = "Up to October 2023"
    rating_reasoning = 3
    rating_speed = 3
    rating_context = 2
    warning = "Smartest"
    date = "May 13, 2024"
    has_json_format = true
    has_json_schema = false
    has_vision = true
    supported_media_types = ["image/jpeg", "image/png", "image/webp", "image/gif"]
    has_function_calling = true

    [llms.providers.Azure.gpt-4-turbo]
    model_name = "GPT-4 Turbo"
    description = "GPT-4 model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens."
    context_window = 128000
    max_output_tokens = 4096
    training_data = "Up to Dec 2023"
    rating_reasoning = 3
    rating_speed = 2
    rating_context = 3
    date = "November 6, 2023"
    has_json_format = true
    has_json_schema = false
    has_vision = true
    supported_media_types = ["image/jpeg", "image/png", "image/webp", "image/gif"]
    has_function_calling = true

    [llms.providers.Azure.default]
    model_id = "gpt-4o"

  llmOpenaiConfig: |
    [llms.providers.OpenAI]
    default_provider = true
    docs_url_slug = "openai"
    display_order = 1


    [llms.providers.OpenAI."o3"]
    model_name = "o3"
    description = "o3 is a large reasoning model, providing high intelligence at the same cost and latency targets."
    context_window = 200000
    rating_reasoning = 3
    rating_speed = 1
    rating_context = 3
    date = "June 10, 2025"
    has_json_format = true
    has_json_schema = true
    has_vision = false
    supported_media_types = []
    has_function_calling = true

    [llms.providers.OpenAI."o4-mini"]
    model_name = "o4 Mini"
    description = "o4-mini is a small reasoning model, providing high intelligence at the same cost and latency targets of o1-mini."
    context_window = 200000
    rating_reasoning = 3
    rating_speed = 2
    rating_context = 3
    date = "April 16, 2025"
    has_json_format = true
    has_json_schema = true
    has_vision = false
    supported_media_types = []
    has_function_calling = true


    [llms.providers.OpenAI."gpt-4.1-2025-04-14"]
    model_name = "GPT 4.1"
    description = "GPT 4.1 model. Supports vision and structured outputs."
    context_window = 1280000
    rating_reasoning = 3
    rating_speed = 2
    rating_context = 3
    warning = "Smartest"
    date = "April 14, 2025"
    has_json_format = true
    has_json_schema = true
    has_vision = true
    supported_media_types = ["image/jpeg", "image/png", "image/webp", "image/gif"]
    has_function_calling = true

    [llms.providers.OpenAI."gpt-4.5-preview"]
    model_name = "GPT 4.5 preview"
    description = "GPT 4.5 preview model. Supports vision and structured outputs."
    context_window = 128000
    rating_reasoning = 3
    rating_speed = 2
    rating_context = 3
    warning = "Smartest"
    date = "February 28, 2025"
    has_json_format = true
    has_json_schema = false
    has_vision = true
    supported_media_types = ["image/jpeg", "image/png", "image/webp", "image/gif"]
    has_function_calling = true

    [llms.providers.OpenAI.gpt-4o-mini]
    model_name = "GPT-4o Mini"
    description = "Light-weight version of GPT 4o. Enables a broad range of tasks with its low cost and latency (perfect for everyday tasks). Supports vision and surpasses GPT-3.5 Turbo performance."
    context_window = 128000
    training_data = "Up to October 2023"
    rating_reasoning = 3
    rating_speed = 3
    rating_context = 2
    warning = "Fastest"
    date = "July 18, 2024"
    has_json_format = true
    has_json_schema = true
    has_vision = true
    supported_media_types = ["image/jpeg", "image/png", "image/webp", "image/gif"]
    has_function_calling = true

    [llms.providers.OpenAI.gpt-4o]
    model_name = "GPT-4o"
    description = "Flagship model from OpenAI's GPT-4 series, best for complex tasks. Supports vision & structured outputs."
    context_window = 128000
    training_data = "Up to October 2023"
    rating_reasoning = 3
    rating_speed = 3
    rating_context = 2
    date = "May 13, 2024"
    has_json_format = true
    has_json_schema = true
    has_vision = true
    supported_media_types = ["image/jpeg", "image/png", "image/webp", "image/gif"]
    has_function_calling = true

    [llms.providers.OpenAI.o3-mini]
    model_name = "o3 Mini"
    description = "o3-mini is a small reasoning model, providing high intelligence at the same cost and latency targets of o1-mini."
    context_window = 200000
    rating_reasoning = 3
    rating_speed = 2
    rating_context = 3
    date = "January 31, 2025"
    has_json_format = true
    has_json_schema = true
    has_vision = false
    supported_media_types = []
    has_function_calling = true

    [llms.providers.OpenAI.gpt-4o-2024-08-06]
    model_name = "GPT-4o 2024-08-06 Snapshot"
    description = "Flagship model from OpenAI's GPT-4 series, best for complex tasks. Supports vision."
    context_window = 128000
    training_data = "Up to Apr 2023"
    rating_reasoning = 3
    rating_speed = 2
    rating_context = 2
    date = "August 6, 2024"
    has_json_format = true
    has_json_schema = true
    has_vision = true
    supported_media_types = ["image/jpeg", "image/png", "image/webp", "image/gif"]
    has_function_calling = true

    [llms.providers.OpenAI.o1-mini]
    model_name = "o1 Mini"
    description = "Optimized for STEM reasoning, especially math and coding. Performs comparably to the larger o1 model on AIME and Codeforces, but runs 80% faster. Ideal for STEM applications, less effective for non-STEM tasks."
    context_window = 128000
    rating_reasoning = 3
    rating_speed = 2
    rating_context = 2
    date = "September 12, 2024"
    has_json_format = false
    has_json_schema = false
    has_vision = false
    supported_media_types = []
    has_function_calling = false

    [llms.providers.OpenAI.default]
    model_id = "gpt-4.1-2025-04-14"

# Persistent Volume Claims
persistence:
  enabled: false
  storageClass: ""
  accessMode: ReadWriteOnce
  size: 8Gi

# Redis configuration
redis:
  enabled: true
  persistence:
    enabled: false
    storageClass: ""
    accessMode: ReadWriteOnce
    size: 1Gi

# Restarter configuration
restarter:
  enabled: true
  image:
    repository: docker
    tag: cli
  schedule: "0 7 * * *" # Daily at 7 AM

# Health checks
healthCheck:
  enabled: true
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3
  successThreshold: 1

# Liveness probe
livenessProbe:
  enabled: true
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3
  successThreshold: 1

# Readiness probe
readinessProbe:
  enabled: true
  initialDelaySeconds: 5
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3
  successThreshold: 1
